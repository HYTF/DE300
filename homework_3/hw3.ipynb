{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. tf-idf definition**"
      ],
      "metadata": {
        "id": "BwvztUyGFqS2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK4jgBLam_KQ",
        "outputId": "ece01f01-3bed-4f20-83b4-2d4a4f2c9881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 33.2M  100 33.2M    0     0   134M      0 --:--:-- --:--:-- --:--:--  134M\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv -O"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .master(\"local[*]\")\n",
        "         .appName(\"AG news\")\n",
        "         .getOrCreate()\n",
        "        )\n",
        "\n",
        "agnews = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"agnews_clean.csv\")\n",
        "\n",
        "# turning the second column from a string to an array\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))"
      ],
      "metadata": {
        "id": "5GFtKwxNnGvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# each row contains the document id and a list of filtered words\n",
        "agnews.show(5, truncate=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZEpTxMNnLGh",
        "outputId": "f32d74ad-2bd9-4010-aed6-ccaf285acd65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------------------+\n",
            "|_c0|                      filtered|\n",
            "+---+------------------------------+\n",
            "|  0|[wall, st, bears, claw, bac...|\n",
            "|  1|[carlyle, looks, toward, co...|\n",
            "|  2|[oil, economy, cloud, stock...|\n",
            "|  3|[iraq, halts, oil, exports,...|\n",
            "|  4|[oil, prices, soar, time, r...|\n",
            "+---+------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agnews.printSchema()\n",
        "agnews.show(3, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOuXdq1enPFc",
        "outputId": "8a4bd7fa-f217-4b4a-96e7-598688a5c360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- filtered: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|_c0|filtered                                                                                                                                                                                                                                          |\n",
            "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0  |[wall, st, bears, claw, back, black, reuters, reuters, short, sellers, wall, street, dwindling, band, ultra, cynics, seeing, green]                                                                                                               |\n",
            "|1  |[carlyle, looks, toward, commercial, aerospace, reuters, reuters, private, investment, firm, carlyle, group, reputation, making, well, timed, occasionally, controversial, plays, defense, industry, quietly, placed, bets, another, part, market]|\n",
            "|2  |[oil, economy, cloud, stocks, outlook, reuters, reuters, soaring, crude, prices, plus, worries, economy, outlook, earnings, expected, hang, stock, market, next, week, depth, summer, doldrums]                                                   |\n",
            "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten csv to (doc id, word)\n",
        "def flatten_doc(row):\n",
        "    doc_id, words = row\n",
        "    return [(doc_id, word) for word in words]\n",
        "\n",
        "rdd = agnews.rdd.map(lambda row: (row['_c0'], row['filtered']))\n",
        "rdd_flattened = rdd.flatMap(flatten_doc)\n",
        "\n",
        "rdd_flattened.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35dPnVe7nS9q",
        "outputId": "3a8bc762-cf59-43c6-f7f9-8736637e1fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'wall'), (0, 'st'), (0, 'bears'), (0, 'claw'), (0, 'back')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Term Frequency (tf)**"
      ],
      "metadata": {
        "id": "mEkjv6wrp3FR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf = (\n",
        "    rdd_flattened\n",
        "    # 0 is document id, 1 is word\n",
        "    .map(lambda x: ((x[0], x[1]), 1))\n",
        "    # count occurrences per (document id, word) pair\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")\n"
      ],
      "metadata": {
        "id": "Xq7FRdRtp9KU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_lengths = (rdd.mapValues(lambda words: len(words))) # gives (document id, total number of words in that doc) for normalizing tf"
      ],
      "metadata": {
        "id": "rEelbGIMimv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_normalized = (\n",
        "    tf.map(lambda x: (x[0][0], (x[0][1], x[1])))# gives (doc_id, (word, tf))\n",
        "    .join(doc_lengths) # gives (doc_id, ((word, tf), doc_lengths))\n",
        "    .map(lambda x: ((x[0], x[1][0][0]), x[1][0][1] / x[1][1])) # gives ((doc_id, word), normalized_tf)\n",
        ")"
      ],
      "metadata": {
        "id": "jY3mGgNui17C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Inverse document frequency (idf)**"
      ],
      "metadata": {
        "id": "TMSochDRrEA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = (\n",
        "    rdd_flattened\n",
        "    .distinct()\n",
        "    .map(lambda x: (x[1], 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")"
      ],
      "metadata": {
        "id": "9Z4QlEWXjDx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "num = rdd.count() # total num of docs\n",
        "idf = df.map(lambda x: (x[0], np.log(num/x[1]))) # gives (word, idf)"
      ],
      "metadata": {
        "id": "5M8y66C2jIAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate tf-idf**"
      ],
      "metadata": {
        "id": "v_FGEZtKrO0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = (\n",
        "    tf_normalized.map(lambda x: (x[0][1], ((x[0][0], x[1])))) # gives (word, (doc_id, tf))\n",
        "    .join(idf) # gives (word, ((doc_id, tf), idf))\n",
        "    .map(lambda x: ((x[1][0][0], x[0]), x[1][0][1] * x[1][1])) # performs multiplication to get tf-idf ((doc_id, word), tf-idf)\n",
        ")\n"
      ],
      "metadata": {
        "id": "DjfF-u_1jRS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving tf-idf measure in a new column**"
      ],
      "metadata": {
        "id": "6tS5SofWrNMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert tifdf rdd to (doc_id, list of (word, tfidf))\n",
        "tfidf_grouped = (\n",
        "    tfidf\n",
        "    .map(lambda x: (x[0][0], (x[0][1], x[1])))\n",
        "    .groupByKey()\n",
        "    .mapValues(list)\n",
        ")"
      ],
      "metadata": {
        "id": "3EySjG3IjYK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a df from tfidf_grouped\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"_c0\", IntegerType(), False),\n",
        "    StructField(\"tfidf\", ArrayType(StructType([\n",
        "        StructField(\"word\", StringType(), False),\n",
        "        StructField(\"tfidf\", DoubleType(), False)\n",
        "    ])))\n",
        "])\n",
        "\n",
        "# convert tuples to dict format\n",
        "tfidf_df_ready = tfidf_grouped.map(lambda x: (x[0], [{\"word\": w, \"tfidf\": float(t)} for w, t in x[1]]))\n",
        "\n",
        "# create df\n",
        "tfidf_df = spark.createDataFrame(tfidf_df_ready, schema)\n"
      ],
      "metadata": {
        "id": "dK9-Qpn9ml_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join on _c0\n",
        "agnews_with_tfidf = agnews.join(tfidf_df, on=\"_c0\", how='left')\n"
      ],
      "metadata": {
        "id": "cFqKE47nm8vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display tf-idf for the first 5 documents**"
      ],
      "metadata": {
        "id": "JI4r9SIJrdHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agnews_with_tfidf.orderBy(\"_c0\").show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUg1HyetnP7t",
        "outputId": "4cf19b32-0b5c-4828-c0e9-aad4d84f4a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|_c0|filtered                                                                                                                                                                                                                                          |tfidf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
            "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0  |[wall, st, bears, claw, back, black, reuters, reuters, short, sellers, wall, street, dwindling, band, ultra, cynics, seeing, green]                                                                                                               |[{black, 0.2953171727366614}, {wall, 0.5115985326511431}, {st, 0.2584728642725166}, {street, 0.24678348986493034}, {dwindling, 0.4572386180709258}, {claw, 0.499114829314058}, {reuters, 0.24754017186645658}, {short, 0.2773120373951269}, {band, 0.3643421454792778}, {ultra, 0.4125512394225831}, {back, 0.1892216338539946}, {green, 0.2877107940095433}, {bears, 0.3372044607529448}, {sellers, 0.4468379768438066}, {cynics, 0.563734318747707}, {seeing, 0.37743394553516213}]                                                                                                                                                                                                                                                                                                                            |\n",
            "|1  |[carlyle, looks, toward, commercial, aerospace, reuters, reuters, private, investment, firm, carlyle, group, reputation, making, well, timed, occasionally, controversial, plays, defense, industry, quietly, placed, bets, another, part, market]|[{part, 0.16022031730914288}, {well, 0.17053284421704767}, {timed, 0.324478643568105}, {occasionally, 0.33274321954270536}, {carlyle, 0.7168306746824437}, {firm, 0.15969712503706046}, {private, 0.1929050573011279}, {investment, 0.1890771769001148}, {bets, 0.27861293130724324}, {reuters, 0.1650267812443044}, {industry, 0.15043731768548949}, {placed, 0.2284965552404658}, {another, 0.14507889141437585}, {defense, 0.1751279339938823}, {market, 0.13394932212703356}, {quietly, 0.25188254045524316}, {group, 0.12468100563149095}, {toward, 0.1898997183872362}, {making, 0.1698717076460444}, {controversial, 0.20949395177306526}, {aerospace, 0.2581171817448437}, {commercial, 0.2057832028092643}, {looks, 0.1973537176743789}, {plays, 0.22418048797172685}, {reputation, 0.2578098186776328}]|\n",
            "|2  |[oil, economy, cloud, stocks, outlook, reuters, reuters, soaring, crude, prices, plus, worries, economy, outlook, earnings, expected, hang, stock, market, next, week, depth, summer, doldrums]                                                   |[{week, 0.13121900794126834}, {summer, 0.22694739048609625}, {plus, 0.24449073714833106}, {outlook, 0.4265073217271922}, {hang, 0.30475018305843793}, {stock, 0.17879168082328206}, {earnings, 0.1792714404894228}, {depth, 0.31343954772064864}, {reuters, 0.18565512889984243}, {oil, 0.13908157105107033}, {expected, 0.16094627131903613}, {stocks, 0.14976769101715193}, {cloud, 0.295159450642955}, {doldrums, 0.3770252270329423}, {prices, 0.14472559202114177}, {market, 0.15069298739291276}, {soaring, 0.2596334462817101}, {economy, 0.3721400726458204}, {crude, 0.197241148492091}, {worries, 0.23009353850726894}, {next, 0.14062721303262238}]                                                                                                                                                   |\n",
            "|3  |[iraq, halts, oil, exports, main, southern, pipeline, reuters, reuters, authorities, halted, oil, export, flows, main, pipeline, southern, iraq, intelligence, showed, rebel, militia, strike, infrastructure, oil, official, said, saturday]     |[{rebel, 0.18209445014364567}, {main, 0.36492623402353547}, {halted, 0.2557691357056513}, {saturday, 0.12197305137253434}, {export, 0.23862435123782139}, {reuters, 0.15913296762843637}, {oil, 0.35763832555989516}, {strike, 0.17411586950893898}, {exports, 0.2146590164054526}, {flows, 0.2774168429760197}, {said, 0.06593367258642661}, {authorities, 0.18159366801541998}, {infrastructure, 0.22959926718225876}, {iraq, 0.23809526243476142}, {intelligence, 0.20782569445751425}, {southern, 0.336553609483104}, {showed, 0.1743365558077232}, {militia, 0.2252006141545402}, {official, 0.15149485319300557}, {pipeline, 0.4720829409342409}, {halts, 0.27365396741681164}]                                                                                                                            |\n",
            "|4  |[oil, prices, soar, time, record, posing, new, menace, us, economy, afp, afp, tearaway, world, oil, prices, toppling, records, straining, wallets, present, new, economic, menace, barely, three, months, us, presidential, elections]            |[{world, 0.09332201126546583}, {toppling, 0.27964532733021175}, {three, 0.10314988960754677}, {months, 0.14002501854271598}, {record, 0.1232987151692413}, {afp, 0.2559170042376607}, {records, 0.19759033440942064}, {oil, 0.22253051368171256}, {new, 0.1271397626254836}, {menace, 0.5747440955975784}, {us, 0.1669859687392097}, {present, 0.22209684830286883}, {barely, 0.21935019724396657}, {presidential, 0.1480257381794347}, {prices, 0.23156094723382684}, {posing, 0.2589223867776184}, {soar, 0.2306791247647116}, {straining, 0.2904044404056468}, {time, 0.10623532598945136}, {economy, 0.14885602905832815}, {tearaway, 0.3918885216630942}, {wallets, 0.2665151844733088}, {economic, 0.14782686453681568}, {elections, 0.16009904796740967}]                                                 |\n",
            "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "CzHRiCdYvl-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. SVM objective function**"
      ],
      "metadata": {
        "id": "dr05iGlYuo8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/w.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/bias.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/data_for_svm.csv -O"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHpNWrNtrBQE",
        "outputId": "bc2fb95b-543b-465d-88f3-39fcba7230fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1391  100  1391    0     0   8763      0 --:--:-- --:--:-- --:--:--  8803\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    22  100    22    0     0    148      0 --:--:-- --:--:-- --:--:--   149\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 61.9M  100 61.9M    0     0  78.3M      0 --:--:-- --:--:-- --:--:-- 78.2M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession.builder\n",
        "         .master(\"local[*]\")\n",
        "         .appName(\"SVM Loss\")\n",
        "         .getOrCreate()\n",
        "        )\n",
        "\n",
        "data_df = spark.read.csv(\"data_for_svm.csv\", inferSchema=True, header=False)\n",
        "X_rdd = data_df.rdd.map(lambda row: np.array(row[:-1]))\n",
        "y_rdd = data_df.rdd.map(lambda row: row[-1])\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "w = pd.read_csv('w.csv', header=None).values.flatten() # 1d numpy array\n",
        "b = pd.read_csv('bias.csv', header=None).values.flatten()[0] # scalar"
      ],
      "metadata": {
        "id": "uJBtL5G0vdxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_SVM(w, b, X, y, lmb=1.0):\n",
        "    \"\"\"\n",
        "    w: numpy array of weights\n",
        "    b: bias\n",
        "    X: RDD of feature vectors\n",
        "    y: RDD of labels\n",
        "    lmb: regularization parameter = 1.0\n",
        "    \"\"\"\n",
        "    # create an RDD of (x_i, y_i) tuples\n",
        "    data_rdd = X.zip(y)\n",
        "\n",
        "    # map to compute hinge loss\n",
        "    hinge_losses = data_rdd.map(lambda xy: max(0, 1 - xy[1] * (np.dot(w, xy[0]) + b)))\n",
        "\n",
        "    # reduce to sum hinge losses\n",
        "    hinge_loss_sum = hinge_losses.reduce(lambda a, b: a + b)\n",
        "\n",
        "    # L2 regularization term\n",
        "    l2 = lmb * np.dot(w, w)\n",
        "\n",
        "    # calculate final loss\n",
        "    loss = l2 + hinge_loss_sum/data_rdd.count()\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "Pk2MzpytxHp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Loss\n",
        "objective_val = loss_SVM(w, b, X_rdd, y_rdd)\n",
        "print(\"The SVM objective value is \",objective_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCW-RQVuyULi",
        "outputId": "7c1567dd-dc3c-4cd7-fe3c-51b80064c287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SVM objective value is  1.0029595550626365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction**"
      ],
      "metadata": {
        "id": "fI0uMjl5xf9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_SVM(w, b, X_rdd):\n",
        "    \"\"\"\n",
        "    w: numpy array of weights\n",
        "    b: bias\n",
        "    X_rdd: RDD of feature vectors\n",
        "\n",
        "    Returns:\n",
        "        RDD of predicted labels\n",
        "    \"\"\"\n",
        "\n",
        "    # compute prediction = sign(w^Tx + b)\n",
        "    predictions = X_rdd.map(lambda x: 1 if np.dot(w, x) + b >= 0 else -1)\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "xtq7dRqFxeT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_rdd = predict_SVM(w, b, X_rdd)\n",
        "predictions = predictions_rdd.collect()\n",
        "print(\"First 10 predictions are: \", predictions[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJh9Ty3eyngm",
        "outputId": "33af2ad7-dd82-4ff1-c76a-e3663d428671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 predictions are:  [-1, -1, -1, 1, -1, 1, -1, -1, 1, -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "eG6xJCV13hts"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}