# Homework 3 - TF-IDF and SVM Loss Calculation

This is a readme file that demonstrates the computation of TF-IDF scores using PySpark MapReduce and calculates the loss for a linear SVM classifier. 

# Part I tf-idf definition
- Download agnews_clean.csv and the expected output is a csv file downloaded in the current working directory
- Start a new spark session and read csv. Turns the second column of the csv from a string to an array for the convenience of later analysis. 
- Show top five rows to take a look at the data format. A table of 5 rows is generated as the output.
+---+------------------------------+
|_c0|                      filtered|
+---+------------------------------+
|  0|[wall, st, bears, claw, bac...|
|  1|[carlyle, looks, toward, co...|
|  2|[oil, economy, cloud, stock...|
|  3|[iraq, halts, oil, exports,...|
|  4|[oil, prices, soar, time, r...|
+---+------------------------------+
only showing top 5 rows
- Print schema of each row and show the top three rows. 
root
 |-- _c0: integer (nullable = true)
 |-- filtered: array (nullable = true)
 |    |-- element: string (containsNull = true)

+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|_c0|filtered                                                                                                                                                                                                                                          |
+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|0  |[wall, st, bears, claw, back, black, reuters, reuters, short, sellers, wall, street, dwindling, band, ultra, cynics, seeing, green]                                                                                                               |
|1  |[carlyle, looks, toward, commercial, aerospace, reuters, reuters, private, investment, firm, carlyle, group, reputation, making, well, timed, occasionally, controversial, plays, defense, industry, quietly, placed, bets, another, part, market]|
|2  |[oil, economy, cloud, stocks, outlook, reuters, reuters, soaring, crude, prices, plus, worries, economy, outlook, earnings, expected, hang, stock, market, next, week, depth, summer, doldrums]                                                   |
+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
only showing top 3 rows

- flatten the csv to (document id, word) by the function flatten_doc, which takes in a row and returns a tuple: one for each word in the document. Extracts two columns from the DataFrame and converts the df to an RDD with structure (doc_id, [word1, word2, ...]). Applies flatten_doc to each row of the RDD, using flatMap to flatten the resulting list of tuples. Now rdd_flattened has the structure: [(doc_id, word), (doc_id, word), ...]. I then printed the first tuples for inspection. The expected output is:
[(0, 'wall'), (0, 'st'), (0, 'bears'), (0, 'claw'), (0, 'back')]
- Next three cells of code is the term frequency computation. Input is rdd_flattened, which has entries like (doc_id, word). Turns each (doc_id, word) into a key and assigns value 1. reduceByKey counts how many times each word appears in a document. Document lengths compute  total number of words in each document. The normalized tf by document length has the step of rearranging tf data to group by doc_id, joining with doc_lengths to get word counts + doc length, and dividing each raw term frequency by the total number of words in that document to normalize.
- Next two chunks of code computes idf. First df is calculated by rdd_flattened: RDD of (doc_id, word), .distinct(): ensures that even if a word appears multiple times in the same doc, and map: to (word, 1) and reduces by key to get the number of documents in which each word appears. idf is computed by the formula provided idf(w) = log(N/df(w))
- tf-idf is computed by rearranging normalized TF data so that it's keyed by word, joining with idf values, and multiplying normalized tf by idf to compute tf-idf score.
- Next four cells of code is to input tf-idf measure in a new column. Th expected output for showing the top five rows is:
+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|_c0|filtered                                                                                                                                                                                                                                          |tfidf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|0  |[wall, st, bears, claw, back, black, reuters, reuters, short, sellers, wall, street, dwindling, band, ultra, cynics, seeing, green]                                                                                                               |[{black, 0.2953171727366614}, {wall, 0.5115985326511431}, {st, 0.2584728642725166}, {street, 0.24678348986493034}, {dwindling, 0.4572386180709258}, {claw, 0.499114829314058}, {reuters, 0.24754017186645658}, {short, 0.2773120373951269}, {band, 0.3643421454792778}, {ultra, 0.4125512394225831}, {back, 0.1892216338539946}, {green, 0.2877107940095433}, {bears, 0.3372044607529448}, {sellers, 0.4468379768438066}, {cynics, 0.563734318747707}, {seeing, 0.37743394553516213}]                                                                                                                                                                                                                                                                                                                            |
|1  |[carlyle, looks, toward, commercial, aerospace, reuters, reuters, private, investment, firm, carlyle, group, reputation, making, well, timed, occasionally, controversial, plays, defense, industry, quietly, placed, bets, another, part, market]|[{part, 0.16022031730914288}, {well, 0.17053284421704767}, {timed, 0.324478643568105}, {occasionally, 0.33274321954270536}, {carlyle, 0.7168306746824437}, {firm, 0.15969712503706046}, {private, 0.1929050573011279}, {investment, 0.1890771769001148}, {bets, 0.27861293130724324}, {reuters, 0.1650267812443044}, {industry, 0.15043731768548949}, {placed, 0.2284965552404658}, {another, 0.14507889141437585}, {defense, 0.1751279339938823}, {market, 0.13394932212703356}, {quietly, 0.25188254045524316}, {group, 0.12468100563149095}, {toward, 0.1898997183872362}, {making, 0.1698717076460444}, {controversial, 0.20949395177306526}, {aerospace, 0.2581171817448437}, {commercial, 0.2057832028092643}, {looks, 0.1973537176743789}, {plays, 0.22418048797172685}, {reputation, 0.2578098186776328}]|
|2  |[oil, economy, cloud, stocks, outlook, reuters, reuters, soaring, crude, prices, plus, worries, economy, outlook, earnings, expected, hang, stock, market, next, week, depth, summer, doldrums]                                                   |[{week, 0.13121900794126834}, {summer, 0.22694739048609625}, {plus, 0.24449073714833106}, {outlook, 0.4265073217271922}, {hang, 0.30475018305843793}, {stock, 0.17879168082328206}, {earnings, 0.1792714404894228}, {depth, 0.31343954772064864}, {reuters, 0.18565512889984243}, {oil, 0.13908157105107033}, {expected, 0.16094627131903613}, {stocks, 0.14976769101715193}, {cloud, 0.295159450642955}, {doldrums, 0.3770252270329423}, {prices, 0.14472559202114177}, {market, 0.15069298739291276}, {soaring, 0.2596334462817101}, {economy, 0.3721400726458204}, {crude, 0.197241148492091}, {worries, 0.23009353850726894}, {next, 0.14062721303262238}]                                                                                                                                                   |
|3  |[iraq, halts, oil, exports, main, southern, pipeline, reuters, reuters, authorities, halted, oil, export, flows, main, pipeline, southern, iraq, intelligence, showed, rebel, militia, strike, infrastructure, oil, official, said, saturday]     |[{rebel, 0.18209445014364567}, {main, 0.36492623402353547}, {halted, 0.2557691357056513}, {saturday, 0.12197305137253434}, {export, 0.23862435123782139}, {reuters, 0.15913296762843637}, {oil, 0.35763832555989516}, {strike, 0.17411586950893898}, {exports, 0.2146590164054526}, {flows, 0.2774168429760197}, {said, 0.06593367258642661}, {authorities, 0.18159366801541998}, {infrastructure, 0.22959926718225876}, {iraq, 0.23809526243476142}, {intelligence, 0.20782569445751425}, {southern, 0.336553609483104}, {showed, 0.1743365558077232}, {militia, 0.2252006141545402}, {official, 0.15149485319300557}, {pipeline, 0.4720829409342409}, {halts, 0.27365396741681164}]                                                                                                                            |
|4  |[oil, prices, soar, time, record, posing, new, menace, us, economy, afp, afp, tearaway, world, oil, prices, toppling, records, straining, wallets, present, new, economic, menace, barely, three, months, us, presidential, elections]            |[{world, 0.09332201126546583}, {toppling, 0.27964532733021175}, {three, 0.10314988960754677}, {months, 0.14002501854271598}, {record, 0.1232987151692413}, {afp, 0.2559170042376607}, {records, 0.19759033440942064}, {oil, 0.22253051368171256}, {new, 0.1271397626254836}, {menace, 0.5747440955975784}, {us, 0.1669859687392097}, {present, 0.22209684830286883}, {barely, 0.21935019724396657}, {presidential, 0.1480257381794347}, {prices, 0.23156094723382684}, {posing, 0.2589223867776184}, {soar, 0.2306791247647116}, {straining, 0.2904044404056468}, {time, 0.10623532598945136}, {economy, 0.14885602905832815}, {tearaway, 0.3918885216630942}, {wallets, 0.2665151844733088}, {economic, 0.14782686453681568}, {elections, 0.16009904796740967}]                                                 |
+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
only showing top 5 rows

- stop the spark session.

# Part II: SVM objective function
- Download data for svm, weights, and bias csv files, and the files will be saved to the current working directory.
- First, start a new spark session and load the csv file data_for_svm.csv where he first 64 columns are the features X and the last column is the label y. Then convert the Spark DataFrame into two RDDs: X_rdd (RDD of NumPy arrays representing feature vectors) and y_rdd (RDD of numeric class labels (e.g., -1 or 1)). Then load pre-trained weight vector w and bias term b from CSV files. 
- SVM loss function first combines X_rdd and y_rdd into a single RDD of tuples (x_i, y_i). Then compute hinge loss for each sample and aggregate it using reduce: max(0,1−yi(w⋅xi+b)). Compute L2 regularization term. Then the final loss is the combination of hinge loss and regularization terms. 
- Objective value is computed by SVM loss function and then printed to the console. The expected output is "The SVM objective value is  1.0029595550626365". 
- Next task is to write a predict_SVM function for the linear SVM model. The function takes in w, b, and X_rdd. It outputs the RDD of predicted labels +1 or -1 using the decision rule: y_i_hat = sgn(w^Tx+b). 
- The predictions are collected and the first ten predictions are printed in the console. The expected output is: "First 10 predictions are:  [-1, -1, -1, 1, -1, 1, -1, -1, 1, -1]". 
- stop the spark session.

# Docker Commands
- docker build .
- docker run -p 8888:8888 \
  -v ~/.aws:/home/jovyan/.aws \
  -v /home/ec2-user/DE300:/home/jovyan/work \
  my_jupyter 
- Dockerfile is included in the directory

# Gen AI Disclosure
- Gen AI (ChatGPT) is used in this assignment. Prompts are the following:
- How to combine two RDDs based on a common key?
- How to flatten a data frame to a 2d array or a scalar?
- Can you explain SVM and its loss function to me?
- Can you help me debug me code... T_T
