# -*- coding: utf-8 -*-
"""linear_regression_DAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U0jJDuR5h0VpthryD0nWohNZgpb2Q6k0
"""

# Import libraries
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
import pandas as pd
import pendulum
from datetime import datetime, timezone, timedelta
import matplotlib.pyplot as plt
import io

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import numpy as np

# Define constants
WEATHER_STATIONS = ["KORD", "KENW", "KMDW", "KPNT"]
BASE_URL = "https://api.weather.gov/stations/{station}/observations/latest"
S3_BUCKET = "benson-gillespie-liu-mwaa"
S3_PREFIX = "predictions"

# Use the same default_args as the lab
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': pendulum.today('UTC').add(days=-1),
    'retries': 1
}

# Define our DAG() function, with a small change in schedule_interval
dag = DAG(
    dag_id="linear_regression_predictions",
    default_args=default_args,
    description="HW4 create predictions dag for Benson, Gillespie, and Liu",
    schedule_interval=timedelta(hours=20),
    tags=['DE300']
)

# Use s3Hook so we can load files from s3 bucket
def predict_temperature():

  s3 = S3Hook(aws_conn_id='aws_default')

  #Set hour mark to 20 for first runthrough
  hour_mark = 20

  for i in range(2):
    # Determine date of the files we want to look at in this format "year_month_day"
    # Subtract a day so we find the previous day in utc
    target_date=(datetime.now(timezone.utc)-timedelta(hours=hour_mark)).strftime('%m_%d_%Y')

    # List all files under the weather_data folder
    keys=s3.list_keys(bucket_name=S3_BUCKET, prefix='weather_data')

    # Create string that we are looking for
    prefix_string=f'date_{target_date}'

    # Create a list called "hour_keys" that finds all keys from the previous desired hours
    hour_keys=[]
    for key in keys:
        if prefix_string in key:
            hour_keys.append(key)

    # Read through each key, use s3.read_key to find the csv content, load into a dataframe
    dfs=[]
    for key in hour_keys:
        csv_content = s3.read_key(key=key, bucket_name=S3_BUCKET)

        # Using io, open the csv content so we aren't reading it as a string, but as a csv
        df=pd.read_csv(io.StringIO(csv_content))
        dfs.append(df)

    # Combine dfs together into one concatenated df
    combined_df=pd.concat(dfs, ignore_index=True)

    # Make sure timestamp is saved as datetime
    combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'])


    # #Trim weather data to only 20 hours before or 40 hours before
    # time = combined_df['timestamp'].max()
    # cutoff_time = time - pd.Timedelta(hours=hour_mark)
    # trim_combined_df = combined_df[combined_df['timestamp'] >= cutoff_time]

    #Split data by station
    station_dfs = {station: group for station, group in combined_df.groupby('station')}

    #Create dataframe for results
    results = [{
        'station': "",
        'thirty_temp_pred': 0,
        'hour_temp_pred': 0,
        'hour_thirty_temp_pred': 0,
        'two_hour_temp_pred': 0,
        'two_hour_thirty_temp_pred': 0,
        'three_hour_temp_pred': 0,
        'three_hour_thirty_temp_pred': 0,
        'four_hour_temp_pred': 0,
        'four_hour_thirty_temp_pred': 0,
        'five_hour_temp_pred': 0,
        'five_hour_thirty_temp_pred': 0,
        'six_hour_temp_pred': 0,
        'six_hour_thirty_temp_pred': 0,
        'seven_hour_temp_pred': 0,
        'seven_hour_thirty_temp_pred': 0,
        'eight_hour_temp_pred': 0
    }]

    results_df = pd.DataFrame(results)

    for station in station_dfs.values():
      station_df = station
      station_name = station_df['station'].iloc[0]
      #Setup Data for model
      for i in range(1, 17):  # 16 half-hour steps
        station_df[f'temp_t+{i}'] = station_df['temperature'].shift(-i)

      #Create lag features to help model identify stronger correlations based on previous temperatures it doesn't "remember"
      station_df['temp_t'] = station_df['temperature']
      station_df['temp_t-1'] = station_df['temperature'].shift(1)

      station_df = station_df.dropna()

      #Split data to predict the 16 future values
      X = station_df[['temp_t','temp_t-1','dewpoint','windSpeed','barometricPressure','visibility','precipitationLastHour','relativeHumidity','heatIndex']]
      y = station_df[[f'temp_t+{i}' for i in range(1, 17)]]

      #Split data for model
      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

      #Create model
      model = LinearRegression()
      model.fit(X_train, y_train)

      #Predict future temperatures
      latest_input = X.iloc[-1].values.reshape(1, -1)
      future_preds = model.predict(latest_input)[0]

      #Place predicted temps into dataframe
      temp_keys = [
          'thirty_temp_pred',
          'hour_temp_pred',
          'hour_thirty_temp_pred',
          'two_hour_temp_pred',
          'two_hour_thirty_temp_pred',
          'three_hour_temp_pred',
          'three_hour_thirty_temp_pred',
          'four_hour_temp_pred',
          'four_hour_thirty_temp_pred',
          'five_hour_temp_pred',
          'five_hour_thirty_temp_pred',
          'six_hour_temp_pred',
          'six_hour_thirty_temp_pred',
          'seven_hour_temp_pred',
          'seven_hour_thirty_temp_pred',
          'eight_hour_temp_pred'
      ]
      new_row = {'station': station_name}
      new_row.update(dict(zip(temp_keys, future_preds)))
      results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)

    #Remove row from dataframe initialization
    results_df = results_df.iloc[1:]

    filename = f"predicted_temperatures_{hour_mark}_hours.csv"

    # Save dataframe as a csv file
    filepath = f"/tmp/{filename}"
    results_df.to_csv(filepath, index=False)

    # Upload csv to our s3 bucket
    s3 = S3Hook(aws_conn_id='aws_default')
    s3.load_file(filename=filepath, key=f"{S3_PREFIX}/{filename}", bucket_name=S3_BUCKET, replace=False)

    #Update hour mark to 40 for second run through
    hour_mark = 40

# Create PythonOperator() task
fetch_and_upload_task = PythonOperator(
    task_id="create_linear_regression_temperature_predictions",
    python_callable=predict_temperature,
    dag=dag,
)